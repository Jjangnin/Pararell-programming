{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile ImageLib.h\n",
        "\n",
        "#pragma once\n",
        "using byte = unsigned char;\n",
        "\n",
        "// 1D/2D/3D/4D 동적할당 & 해제\n",
        "double *dmatrix1D(int nH);\n",
        "double **dmatrix2D(int nH, int nW);\n",
        "double ***dmatrix3D(int nH, int nW, int nC);\n",
        "double ****dmatrix4D(int nH, int nW, int nC, int nNum);\n",
        "\n",
        "void free_dmatrix1D(double *Image, int nH);\n",
        "void free_dmatrix2D(double **Image, int nH, int nW);\n",
        "void free_dmatrix3D(double ***Image, int nH, int nW, int nC);\n",
        "void free_dmatrix4D(double ****Image, int nH, int nW, int nC, int nNum);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzOO0P23OyB9",
        "outputId": "70419eb5-0471-4b4d-8c72-6bf2ff5152dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ImageLib.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ImageLib.cpp\n",
        "\n",
        "#include \"ImageLib.h\"\n",
        "\n",
        "double *dmatrix1D(int nH) {\n",
        "    return new double[nH]();\n",
        "}\n",
        "\n",
        "double **dmatrix2D(int nH, int nW) {\n",
        "    double **Temp = new double*[nH];\n",
        "    for (int y = 0; y < nH; ++y) {\n",
        "        Temp[y] = new double[nW]();\n",
        "    }\n",
        "    return Temp;\n",
        "}\n",
        "\n",
        "double ***dmatrix3D(int nH, int nW, int nC) {\n",
        "    double ***Temp = new double**[nH];\n",
        "    for (int y = 0; y < nH; ++y) {\n",
        "        Temp[y] = new double*[nW];\n",
        "        for (int x = 0; x < nW; ++x) {\n",
        "            Temp[y][x] = new double[nC]();\n",
        "        }\n",
        "    }\n",
        "    return Temp;\n",
        "}\n",
        "\n",
        "double ****dmatrix4D(int nH, int nW, int nC, int nNum) {\n",
        "    double ****Temp = new double***[nH];\n",
        "    for (int y = 0; y < nH; ++y) {\n",
        "        Temp[y] = new double**[nW];\n",
        "        for (int x = 0; x < nW; ++x) {\n",
        "            Temp[y][x] = new double*[nC];\n",
        "            for (int c = 0; c < nC; ++c) {\n",
        "                Temp[y][x][c] = new double[nNum]();\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return Temp;\n",
        "}\n",
        "\n",
        "void free_dmatrix1D(double *Image, int) { delete[] Image; }\n",
        "\n",
        "void free_dmatrix2D(double **Image, int nH, int) {\n",
        "    for (int y = 0; y < nH; ++y) delete[] Image[y];\n",
        "    delete[] Image;\n",
        "}\n",
        "\n",
        "void free_dmatrix3D(double ***Image, int nH, int nW, int) {\n",
        "    for (int y = 0; y < nH; ++y) {\n",
        "        for (int x = 0; x < nW; ++x) delete[] Image[y][x];\n",
        "        delete[] Image[y];\n",
        "    }\n",
        "    delete[] Image;\n",
        "}\n",
        "\n",
        "void free_dmatrix4D(double ****Image, int nH, int nW, int nC, int) {\n",
        "    for (int y = 0; y < nH; ++y) {\n",
        "        for (int x = 0; x < nW; ++x) {\n",
        "            for (int c = 0; c < nC; ++c) delete[] Image[y][x][c];\n",
        "            delete[] Image[y][x];\n",
        "        }\n",
        "        delete[] Image[y];\n",
        "    }\n",
        "    delete[] Image;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyCK5YwBPaBe",
        "outputId": "50fad09f-c785-4d4c-bfcb-6e1b522fb472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ImageLib.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CTensor.h\n",
        "\n",
        "#pragma once\n",
        "#include \"ImageLib.h\"\n",
        "#include <iostream>\n",
        "\n",
        "/// Tensor3D는 크기가 (nH x nW x nC)인 3차원 tensor를 관리함\n",
        "class Tensor3D {\n",
        "private:\n",
        "    double*** tensor;\n",
        "    int nH; // height\n",
        "    int nW; // width\n",
        "    int nC; // channel\n",
        "\n",
        "public:\n",
        "    // 동작: 1) 3차원 행렬 동적할당 후 tensor에 시작 주소 저장\n",
        "    //       2) 모든 element 0으로 초기화\n",
        "    // 사용함수: dmatrix3D()\n",
        "    Tensor3D(int _nH, int _nW, int _nC) : tensor(nullptr), nH(_nH), nW(_nW), nC(_nC) {\n",
        "        tensor = dmatrix3D(nH, nW, nC);\n",
        "        // 0으로 초기화\n",
        "        for (int h = 0; h < nH; ++h) {\n",
        "            for (int w = 0; w < nW; ++w) {\n",
        "                for (int c = 0; c < nC; ++c) {\n",
        "                    tensor[h][w][c] = 0.0;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // 동작: 3차원 동적 배열 할당 해제\n",
        "    // 사용함수: free_dmatrix3D()\n",
        "    ~Tensor3D() {\n",
        "        if (tensor) {\n",
        "            free_dmatrix3D(tensor, nH, nW, nC);\n",
        "            tensor = nullptr;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // 특정 위치 원소 설정/조회\n",
        "    inline void set_elem(int _h, int _w, int _c, double _val) { tensor[_h][_w][_c] = _val; }\n",
        "\n",
        "    // 동작: 행=_h, 열=_w, 채널=_c 위치 element 반환\n",
        "    inline double get_elem(int _h, int _w, int _c) const {\n",
        "        return tensor[_h][_w][_c];\n",
        "    }\n",
        "\n",
        "    // 동작: 행렬의 차원(nH, nW, nC)을 pass by reference로 반환\n",
        "    inline void get_info(int& _nH, int& _nW, int& _nC) const {\n",
        "        _nH = nH; _nW = nW; _nC = nC;\n",
        "    }\n",
        "\n",
        "    inline void set_tensor(double*** _tensor) { tensor = _tensor; }\n",
        "    inline double*** get_tensor() const { return tensor; }\n",
        "\n",
        "    // 동작: 행렬의 크기 (nH*nW*nC)를 화면에 출력\n",
        "    void print() const {\n",
        "        std::cout << \"Tensor3D size: \"\n",
        "                  << nH << \" x \" << nW << \" x \" << nC\n",
        "                  << \" (\" << static_cast<long long>(nH) * nW * nC << \" elements)\"\n",
        "                  << std::endl;\n",
        "    }\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IWYHFV-POvf",
        "outputId": "79c75663-a5c6-45cd-f2a2-6afb14394557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CTensor.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XMBiLcQIl8O",
        "outputId": "2f1c3c3d-0f9a-44e7-d747-48137e1859f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CLayer.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile CLayer.h\n",
        "#pragma once\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <stdexcept>\n",
        "#include <string>\n",
        "#include \"ImageLib.h\"\n",
        "#include \"CTensor.h\"\n",
        "\n",
        "#define MEAN_INIT 0\n",
        "#define LOAD_INIT 1\n",
        "\n",
        "using std::cout;\n",
        "using std::endl;\n",
        "using std::string;\n",
        "\n",
        "class Layer {\n",
        "protected:\n",
        "    int fK;\n",
        "    int fC_in;\n",
        "    int fC_out;\n",
        "    string name;\n",
        "public:\n",
        "    Layer(string _name, int _fK, int _fC_in, int _fC_out)\n",
        "        : name(_name), fK(_fK), fC_in(_fC_in), fC_out(_fC_out) {}\n",
        "    virtual ~Layer() {}\n",
        "    virtual Tensor3D* forward(const Tensor3D* input) = 0;\n",
        "    virtual void print() const = 0;\n",
        "    virtual void get_info(string& _name, int& _fK, int& _fC_in, int& _fC_out) const = 0;\n",
        "\n",
        "    // 병렬 스위치\n",
        "    virtual void set_parallel(bool) {}\n",
        "};\n",
        "\n",
        "class Layer_ReLU : public Layer {\n",
        "    bool use_parallel = false;\n",
        "public:\n",
        "    Layer_ReLU(string _name, int _fK, int _fC_in, int _fC_out)\n",
        "        : Layer(_name, _fK, _fC_in, _fC_out) {}\n",
        "    ~Layer_ReLU() {}\n",
        "\n",
        "    void set_parallel(bool on) override { use_parallel = on; }\n",
        "\n",
        "    Tensor3D* forward(const Tensor3D* input) override {\n",
        "        int nH, nW, nC;\n",
        "        input->get_info(nH, nW, nC);\n",
        "        Tensor3D* output = new Tensor3D(nH, nW, nC);\n",
        "\n",
        "        if (!use_parallel) {\n",
        "            for (int h = 0; h < nH; ++h)\n",
        "                for (int w = 0; w < nW; ++w)\n",
        "                    for (int c = 0; c < nC; ++c) {\n",
        "                        double x = input->get_elem(h, w, c);\n",
        "                        output->set_elem(h, w, c, x > 0.0 ? x : 0.0);\n",
        "                    }\n",
        "        } else {\n",
        "            #ifdef _OPENMP\n",
        "            #pragma omp parallel for collapse(3) schedule(static)\n",
        "            #endif\n",
        "            for (int c = 0; c < nC; ++c)\n",
        "                for (int h = 0; h < nH; ++h)\n",
        "                    for (int w = 0; w < nW; ++w) {\n",
        "                        double x = input->get_elem(h, w, c);\n",
        "                        output->set_elem(h, w, c, x > 0.0 ? x : 0.0);\n",
        "                    }\n",
        "        }\n",
        "\n",
        "        cout << name << \" is finished\" << endl;\n",
        "        return output;\n",
        "    }\n",
        "\n",
        "    void get_info(string& _name, int& _fK, int& _fC_in, int& _fC_out) const override {\n",
        "        _name = name; _fK = fK; _fC_in = fC_in; _fC_out = fC_out;\n",
        "    }\n",
        "\n",
        "    void print() const override {\n",
        "        cout << \"[Layer_ReLU] \" << name\n",
        "             << \" | fK=\" << fK\n",
        "             << \" | C_in=\" << fC_in\n",
        "             << \" | C_out=\" << fC_out\n",
        "             << endl;\n",
        "    }\n",
        "};\n",
        "\n",
        "class Layer_Conv : public Layer {\n",
        "private:\n",
        "    string filename_weight;\n",
        "    string filename_bias;\n",
        "    double**** weight_tensor; // [fK][fK][fC_in][fC_out]\n",
        "    double*   bias_tensor;    // [fC_out]\n",
        "    bool use_parallel = false;\n",
        "\n",
        "    void init(int init_type) {\n",
        "        weight_tensor = dmatrix4D(fK, fK, fC_in, fC_out);\n",
        "        bias_tensor   = dmatrix1D(fC_out);\n",
        "\n",
        "        if (init_type == MEAN_INIT) {\n",
        "            const double val = 1.0 / static_cast<double>(fK * fK * fC_in);\n",
        "            for (int kh=0; kh<fK; ++kh)\n",
        "                for (int kw=0; kw<fK; ++kw)\n",
        "                    for (int ic=0; ic<fC_in; ++ic)\n",
        "                        for (int oc=0; oc<fC_out; ++oc)\n",
        "                            weight_tensor[kh][kw][ic][oc] = val;\n",
        "            for (int oc=0; oc<fC_out; ++oc) bias_tensor[oc] = 0.0;\n",
        "        } else if (init_type == LOAD_INIT) {\n",
        "            if (filename_weight.empty() || filename_bias.empty())\n",
        "                throw std::runtime_error(\"LOAD_INIT requires valid weight/bias filenames.\");\n",
        "\n",
        "            std::ifstream fw(filename_weight);\n",
        "            if (!fw.is_open()) throw std::runtime_error(\"Failed to open weight file: \" + filename_weight);\n",
        "            for (int kh=0; kh<fK; ++kh)\n",
        "                for (int kw=0; kw<fK; ++kw)\n",
        "                    for (int ic=0; ic<fC_in; ++ic)\n",
        "                        for (int oc=0; oc<fC_out; ++oc) {\n",
        "                            double v;\n",
        "                            if (!(fw >> v)) throw std::runtime_error(\"Invalid weight data in \" + filename_weight);\n",
        "                            weight_tensor[kh][kw][ic][oc] = v;\n",
        "                        }\n",
        "            fw.close();\n",
        "\n",
        "            std::ifstream fb(filename_bias);\n",
        "            if (!fb.is_open()) throw std::runtime_error(\"Failed to open bias file: \" + filename_bias);\n",
        "            for (int oc=0; oc<fC_out; ++oc) {\n",
        "                double b;\n",
        "                if (!(fb >> b)) throw std::runtime_error(\"Invalid bias data in \" + filename_bias);\n",
        "                bias_tensor[oc] = b;\n",
        "            }\n",
        "            fb.close();\n",
        "        } else {\n",
        "            throw std::runtime_error(\"Unknown init_type for Layer_Conv\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "public:\n",
        "    Layer_Conv(string _name, int _fK, int _fC_in, int _fC_out, int init_type,\n",
        "               string _filename_weight = \"\", string _filename_bias = \"\")\n",
        "        : Layer(_name, _fK, _fC_in, _fC_out),\n",
        "          filename_weight(_filename_weight),\n",
        "          filename_bias(_filename_bias),\n",
        "          weight_tensor(nullptr),\n",
        "          bias_tensor(nullptr)\n",
        "    {\n",
        "        init(init_type);\n",
        "    }\n",
        "\n",
        "    ~Layer_Conv() override {\n",
        "        if (weight_tensor) { free_dmatrix4D(weight_tensor, fK, fK, fC_in, fC_out); weight_tensor = nullptr; }\n",
        "        if (bias_tensor)   { free_dmatrix1D(bias_tensor, fC_out); bias_tensor = nullptr; }\n",
        "    }\n",
        "\n",
        "    void set_parallel(bool on) override { use_parallel = on; }\n",
        "\n",
        "    Tensor3D* forward(const Tensor3D* input) override {\n",
        "        int inH, inW, inC;\n",
        "        input->get_info(inH, inW, inC);\n",
        "        if (inC != fC_in) throw std::runtime_error(\"Conv: input channel mismatch.\");\n",
        "        if (inH < fK || inW < fK) throw std::runtime_error(\"Conv: input smaller than kernel.\");\n",
        "\n",
        "        const int outH = inH - fK + 1;\n",
        "        const int outW = inW - fK + 1;\n",
        "        Tensor3D* output = new Tensor3D(outH, outW, fC_out);\n",
        "\n",
        "        if (!use_parallel) {\n",
        "            for (int h=0; h<outH; ++h){\n",
        "                for (int w=0; w<outW; ++w){\n",
        "                    for (int oc=0; oc<fC_out; ++oc){\n",
        "                        double acc = 0.0;\n",
        "                        for (int kh=0; kh<fK; ++kh)\n",
        "                            for (int kw=0; kw<fK; ++kw)\n",
        "                                for (int ic=0; ic<fC_in; ++ic){\n",
        "                                    double x = input->get_elem(h+kh, w+kw, ic);\n",
        "                                    acc += weight_tensor[kh][kw][ic][oc] * x;\n",
        "                                }\n",
        "                        acc += bias_tensor[oc];\n",
        "                        output->set_elem(h, w, oc, acc);\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        } else {\n",
        "            #ifdef _OPENMP\n",
        "            #pragma omp parallel for collapse(3) schedule(static)\n",
        "            #endif\n",
        "            for (int oc=0; oc<fC_out; ++oc){\n",
        "                for (int h=0; h<outH; ++h){\n",
        "                    for (int w=0; w<outW; ++w){\n",
        "                        double acc = 0.0;\n",
        "                        for (int ic=0; ic<fC_in; ++ic)\n",
        "                            for (int kh=0; kh<fK; ++kh)\n",
        "                                for (int kw=0; kw<fK; ++kw){\n",
        "                                    double x = input->get_elem(h+kh, w+kw, ic);\n",
        "                                    acc += weight_tensor[kh][kw][ic][oc] * x;\n",
        "                                }\n",
        "                        acc += bias_tensor[oc];\n",
        "                        output->set_elem(h, w, oc, acc);\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        cout << name << \" is finished\" << endl;\n",
        "        return output;\n",
        "    }\n",
        "\n",
        "    void get_info(string& _name, int& _fK, int& _fC_in, int& _fC_out) const override {\n",
        "        _name = name; _fK = fK; _fC_in = fC_in; _fC_out = fC_out;\n",
        "    }\n",
        "\n",
        "    void print() const override {\n",
        "        cout << \"[Layer_Conv] \" << name\n",
        "             << \" | fK=\" << fK\n",
        "             << \" | C_in=\" << fC_in\n",
        "             << \" | C_out=\" << fC_out\n",
        "             << \" | stride=1, padding=0\" << endl;\n",
        "    }\n",
        "};"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "4o3T6qtNO6MS",
        "outputId": "e99d1e23-303e-40e2-b3e8-22a92e1a527c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ddff4e29-4f17-4ae0-8db7-a61e34821901\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ddff4e29-4f17-4ae0-8db7-a61e34821901\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tensor_5x5x2.txt to tensor_5x5x2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cpp\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <iomanip>\n",
        "#include <string>\n",
        "#include <thread>\n",
        "#include <chrono>\n",
        "#ifdef _OPENMP\n",
        "#include <omp.h>\n",
        "#endif\n",
        "#include \"ImageLib.h\"\n",
        "#include \"CLayer.h\"\n",
        "using namespace std;\n",
        "\n",
        "void print_all_elements(const Tensor3D& tensor) {\n",
        "    int nH, nW, nC;\n",
        "    tensor.get_info(nH, nW, nC);\n",
        "    cout.precision(3);\n",
        "    for (int c = 0; c < nC; c++) {\n",
        "        cout << c << \"-th channel:\" << endl;\n",
        "        for (int h = 0; h < nH; h++) {\n",
        "            for (int w = 0; w < nW; w++) {\n",
        "                cout << setw(8) << tensor.get_elem(h, w, c);\n",
        "            }\n",
        "            cout << endl;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "Tensor3D* read_tensor(string filename) {\n",
        "    ifstream fin(filename);\n",
        "    if (!fin.is_open()) {\n",
        "        cerr << \"Failed to open: \" << filename << endl;\n",
        "        return nullptr;\n",
        "    }\n",
        "    int fK = 5;\n",
        "    int fC = 2;\n",
        "    Tensor3D* temp = new Tensor3D(fK, fK, fC);\n",
        "    for (int c = 0; c < fC; c++) {\n",
        "        for (int w = 0; w < fK; w++) {\n",
        "            for (int h = 0; h < fK; h++) {\n",
        "                double val;\n",
        "                fin >> val;\n",
        "                temp->set_elem(h, w, c, val);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    fin.close();\n",
        "    return temp;\n",
        "}\n",
        "\n",
        "void print_hw_info() {\n",
        "    cout << \"===== Hardware Info =====\" << endl;\n",
        "    cout << \"std::thread::hardware_concurrency(): \" << thread::hardware_concurrency() << endl;\n",
        "#ifdef _OPENMP\n",
        "    cout << \"OpenMP available: yes\" << endl;\n",
        "    cout << \"omp_get_max_threads(): \" << omp_get_max_threads() << endl;\n",
        "    cout << \"omp_get_num_procs(): \" << omp_get_num_procs() << endl;\n",
        "#else\n",
        "    cout << \"OpenMP available: no (compile with -fopenmp)\" << endl;\n",
        "#endif\n",
        "    cout << \"=========================\" << endl << endl;\n",
        "}\n",
        "\n",
        "template<typename Fn>\n",
        "double time_ms(Fn&& fn) {\n",
        "    auto t0 = chrono::high_resolution_clock::now();\n",
        "    fn();\n",
        "    auto t1 = chrono::high_resolution_clock::now();\n",
        "    return chrono::duration<double, milli>(t1 - t0).count();\n",
        "}\n",
        "\n",
        "void bench_fill_serial(Tensor3D& t) {\n",
        "    int H,W,C; t.get_info(H,W,C);\n",
        "    for (int h=0; h<H; ++h)\n",
        "        for (int w=0; w<W; ++w)\n",
        "            for (int c=0; c<C; ++c)\n",
        "                t.set_elem(h,w,c, (h+1)*0.001 + (w+1)*0.002 + (c+1)*0.003);\n",
        "}\n",
        "\n",
        "void bench_fill_parallel(Tensor3D& t) {\n",
        "    int H,W,C; t.get_info(H,W,C);\n",
        "#ifdef _OPENMP\n",
        "    #pragma omp parallel for collapse(3) schedule(static)\n",
        "#endif\n",
        "    for (int h=0; h<H; ++h)\n",
        "        for (int w=0; w<W; ++w)\n",
        "            for (int c=0; c<C; ++c)\n",
        "                t.set_elem(h,w,c, (h+1)*0.001 + (w+1)*0.002 + (c+1)*0.003);\n",
        "}\n",
        "\n",
        "double bench_sum_serial(const Tensor3D& t) {\n",
        "    int H,W,C; t.get_info(H,W,C);\n",
        "    double s=0.0;\n",
        "    for (int h=0; h<H; ++h)\n",
        "        for (int w=0; w<W; ++w)\n",
        "            for (int c=0; c<C; ++c)\n",
        "                s += t.get_elem(h,w,c);\n",
        "    return s;\n",
        "}\n",
        "\n",
        "double bench_sum_parallel(const Tensor3D& t) {\n",
        "    int H,W,C; t.get_info(H,W,C);\n",
        "    double s=0.0;\n",
        "#ifdef _OPENMP\n",
        "    #pragma omp parallel for collapse(3) reduction(+:s) schedule(static)\n",
        "#endif\n",
        "    for (int h=0; h<H; ++h)\n",
        "        for (int w=0; w<W; ++w)\n",
        "            for (int c=0; c<C; ++c)\n",
        "                s += t.get_elem(h,w,c);\n",
        "    return s;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    print_hw_info();\n",
        "\n",
        "    Layer* layer1 = new Layer_Conv(\"Conv1\", 3, 2, 1, MEAN_INIT);\n",
        "    Layer* layer2 = new Layer_ReLU(\"Relu1\", 1, 1, 1);\n",
        "\n",
        "    cout << \"(Layer information)________________\" << endl;\n",
        "    layer1->print();\n",
        "    layer2->print();\n",
        "    cout << endl;\n",
        "\n",
        "    Tensor3D* tensor1 = read_tensor(\"tensor_5x5x2.txt\");\n",
        "    if (!tensor1) return 1;\n",
        "\n",
        "    layer1->set_parallel(false);\n",
        "    layer2->set_parallel(false);\n",
        "    Tensor3D *tensor2_s = nullptr, *tensor3_s = nullptr;\n",
        "    (void)time_ms([&](){\n",
        "        tensor2_s = layer1->forward(tensor1);\n",
        "        tensor3_s = layer2->forward(tensor2_s);\n",
        "    });\n",
        "\n",
        "    layer1->set_parallel(true);\n",
        "    layer2->set_parallel(true);\n",
        "    Tensor3D *tensor2_p = nullptr, *tensor3_p = nullptr;\n",
        "    (void)time_ms([&](){\n",
        "        tensor2_p = layer1->forward(tensor1);\n",
        "        tensor3_p = layer2->forward(tensor2_p);\n",
        "    });\n",
        "\n",
        "    cout << \"\\n(Tensor information)________________\" << endl;\n",
        "\n",
        "    cout << \"[tensor1]:\" << endl;\n",
        "    tensor1->print();\n",
        "    print_all_elements((*tensor1));\n",
        "    cout << endl;\n",
        "\n",
        "    cout << \"[tensor2]:\" << endl;\n",
        "    tensor2_s->print();\n",
        "    print_all_elements((*tensor2_s));\n",
        "    cout << endl;\n",
        "\n",
        "    cout << \"[tensor3]:\" << endl;\n",
        "    tensor3_s->print();\n",
        "    print_all_elements((*tensor3_s));\n",
        "    cout << endl;\n",
        "\n",
        "    const int BH=1024, BW=2048, BC=16;\n",
        "    Tensor3D bench(BH,BW,BC);\n",
        "\n",
        "    auto ms_fill_s = time_ms([&]{ bench_fill_serial(bench); });\n",
        "    double sum_s   = 0.0;\n",
        "    auto ms_sum_s  = time_ms([&]{ sum_s = bench_sum_serial(bench); });\n",
        "\n",
        "    auto ms_fill_p = time_ms([&]{ bench_fill_parallel(bench); });\n",
        "    double sum_p   = 0.0;\n",
        "    auto ms_sum_p  = time_ms([&]{ sum_p = bench_sum_parallel(bench); });\n",
        "\n",
        "    cout.setf(std::ios::fixed);\n",
        "    cout << \"===== Benchmark Tensor =====\" << endl;\n",
        "    cout << \"Size: \" << BH << \" x \" << BW << \" x \" << BC << endl;\n",
        "    cout << setprecision(3);\n",
        "    cout << \"Serial  : fill \" << ms_fill_s << \" ms, sum \" << ms_sum_s\n",
        "         << \" ms, total \" << (ms_fill_s + ms_sum_s) << \" ms\" << endl;\n",
        "    cout << \"Parallel: fill \" << ms_fill_p << \" ms, sum \" << ms_sum_p\n",
        "         << \" ms, total \" << (ms_fill_p + ms_sum_p) << \" ms\" << endl;\n",
        "    cout << setprecision(6);\n",
        "    cout << \"sum(serial) = \"   << sum_s << \", sum(parallel) = \" << sum_p << endl;\n",
        "    cout << setprecision(3);\n",
        "    double speedup = (ms_fill_s + ms_sum_s) / (ms_fill_p + ms_sum_p);\n",
        "    cout << \"Speedup (Serial/Parallel): \" << speedup << \"x\" << endl;\n",
        "\n",
        "    delete tensor1;\n",
        "    delete tensor2_s; delete tensor3_s;\n",
        "    delete tensor2_p; delete tensor3_p;\n",
        "    delete layer1; delete layer2;\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT9e4ELZOpST",
        "outputId": "91393a3f-7d36-474b-830d-2121444a2329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "g++ -O3 -march=native -fopenmp main.cpp ImageLib.cpp -o run\n",
        "./run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEDji8ShPAD-",
        "outputId": "02dbea6f-5da6-460d-bdf6-bc2b6236ec1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Hardware Info =====\n",
            "std::thread::hardware_concurrency(): 2\n",
            "OpenMP available: yes\n",
            "omp_get_max_threads(): 2\n",
            "omp_get_num_procs(): 2\n",
            "=========================\n",
            "\n",
            "(Layer information)________________\n",
            "[Layer_Conv] Conv1 | fK=3 | C_in=2 | C_out=1 | stride=1, padding=0\n",
            "[Layer_ReLU] Relu1 | fK=1 | C_in=1 | C_out=1\n",
            "\n",
            "Conv1 is finished\n",
            "Relu1 is finished\n",
            "Conv1 is finished\n",
            "Relu1 is finished\n",
            "\n",
            "(Tensor information)________________\n",
            "[tensor1]:\n",
            "Tensor3D size: 5 x 5 x 2 (50 elements)\n",
            "0-th channel:\n",
            "   0.694   0.609  -0.151  -0.252 -0.0679\n",
            "   0.505   0.538    0.64   0.506    0.64\n",
            "   0.267  0.0882  -0.457   0.197   0.255\n",
            "    0.26  -0.654  -0.505  -0.671  -0.333\n",
            " -0.0268  -0.148   0.301-0.00506  0.0339\n",
            "1-th channel:\n",
            "   0.307   0.615  -0.336   0.215     0.2\n",
            "    0.25  -0.097  -0.403   0.279  -0.298\n",
            "  -0.502  -0.232 -0.0643  -0.657   0.158\n",
            "    0.23   0.299   -0.63  -0.651  -0.412\n",
            "   0.579   -0.61  -0.309  -0.245   -0.11\n",
            "\n",
            "[tensor2]:\n",
            "Tensor3D size: 3 x 3 x 1 (9 elements)\n",
            "0-th channel:\n",
            "   0.126  0.0577  0.0225\n",
            "  -0.026  -0.137  -0.134\n",
            "  -0.117  -0.275  -0.228\n",
            "\n",
            "[tensor3]:\n",
            "Tensor3D size: 3 x 3 x 1 (9 elements)\n",
            "0-th channel:\n",
            "   0.126  0.0577  0.0225\n",
            "       0       0       0\n",
            "       0       0       0\n",
            "\n",
            "===== Benchmark Tensor =====\n",
            "Size: 1024 x 2048 x 16\n",
            "Serial  : fill 33.916 ms, sum 56.480 ms, total 90.397 ms\n",
            "Parallel: fill 159.044 ms, sum 56.309 ms, total 215.354 ms\n",
            "sum(serial) = 86805315.584000, sum(parallel) = 86805315.584000\n",
            "Speedup (Serial/Parallel): 0.420x\n"
          ]
        }
      ]
    }
  ]
}